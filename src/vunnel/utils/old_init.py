# pylint: skip-file

import copy
import importlib
import json
import os
import re
import time
from collections import namedtuple

from anchore_engine.configuration.localconfig import get_config
from anchore_engine.db import session_scope
from anchore_engine.subsys import logger
from anchore_engine.subsys.metrics import gauge_set, summary_observe
from anchore_enterprise.db.entities.feeds import DataRecord, Feed, Group, Task
from anchore_enterprise.services.feeds.drivers import common, tasks
from anchore_enterprise.services.feeds.drivers.grypedb import data as grype_db_driver
from pkg_resources import resource_filename
from sqlalchemy.orm.exc import MultipleResultsFound

_drivers_initialized = False
_driver_modules = {}

PayloadSummary = namedtuple("PayloadSummary", ["key", "namespace"])
DataRecordSummary = namedtuple("DataRecordSummary", ["record_id", "group_id"])


def init_drivers(skip_if_exists=False, local_workspace="/tmp/feeds_workspace"):
    global _drivers_initialized, _driver_modules

    try:
        common.init_workspace(local_workspace=local_workspace)
    except Exception as err:
        logger.exception("Failed to initialize driver workspace at {}".format(local_workspace))
        raise err

    try:
        driverdir = resource_filename("anchore_enterprise", "services/feeds/drivers")
        moduleform = "anchore_enterprise.services.feeds.drivers"

        logger.info("loading drivers")
        logger.debug("discovered driver dir: " + str(driverdir) + " moduleform: " + str(moduleform))
        for f in os.listdir(driverdir):
            if os.path.isdir(os.path.join(driverdir, f)) and os.path.exists(os.path.join(driverdir, f, "data.py")):
                driver = re.sub(r".py", "", f)
                try:
                    themodule = importlib.import_module(".".join([moduleform, driver, "data"]))
                    try:
                        namespace = themodule.namespace
                    except:
                        namespace = driver
                    try:
                        feedtype = themodule.feedtype
                    except:
                        feedtype = "general"

                    try:
                        purge_unreported = themodule.purge_unreported
                    except:
                        purge_unreported = False

                    _driver_modules[driver] = {
                        "namespace": namespace,
                        "feedtype": feedtype,
                        "module": importlib.import_module(".".join([moduleform, driver, "data"])),
                        "purge_unreported": purge_unreported,
                    }
                    logger.info("driver loaded: " + str(driver))
                except Exception as err:
                    logger.warn("driver failed to load, will not execute - exception: " + str(err))
    except Exception as err:
        logger.error("error loading driver modules - exception: " + str(err))
        raise err

    _drivers_initialized = True


def run_drivers(skip_if_exists=False):
    global _drivers_initialized, _driver_modules

    # get driver config
    try:
        config = get_config().get("services", {}).get("feeds", {})
        driver_config = config.get("drivers", {})
    except:
        logger.warn("Failed to parse driver config, continuing feed sync task with defaults")
        config = {}
        driver_config = {}

    if str(config.get("api_only", False)).lower() in ["true", "t"]:
        logger.info("api_only set to True in config. Skipping feed sync")
        return

    # start a task for feed sync
    p_task_id = tasks.create_feed_sync_task()
    if not p_task_id:
        logger.warn("Aborting feeds sync operation due to an invalid task ID")
        return

    logger.info("Starting feed sync task {}".format(p_task_id))

    # check and configure workspace
    try:
        common.configure_workspace(feed_sync_task_id=p_task_id)
    except:
        logger.exception("Aborting feeds sync operation due to error configuring workspace")
        tasks.update_task(
            p_task_id,
            tasks.TaskStatus.FAILED.value,
            result={"error": "Aborting feeds sync operation due to error configuring workspace"},
        )
        return

    # this is to force nvdv2 to run first
    driver_keys = ["nvdv2"]
    for driver_key in _driver_modules:
        if driver_key not in driver_keys:
            driver_keys.append(driver_key)

    # ensure that grypedb driver is final one to drive because of its dependency on data generated by other drivers
    if "grypedb" in driver_keys:
        driver_keys.append(driver_keys.pop(driver_keys.index("grypedb")))

    persisted_feeds = []  # local cache for persisted feed ids, each feed is represented by feed_id string
    completed_drivers = []
    failed_drivers = []
    skipped_drivers = []

    # Metrics stuff
    query_timer_name = "anchore_feeds_driver_db_lookup"
    flush_timer_name = "anchore_feeds_driver_db_flush"
    driver_timer_name = "anchore_feeds_driver_duration"
    task_failures_name = "anchore_feeds_task_failures"
    task_success_name = "anchore_feeds_task_success"

    for driver in driver_keys:
        persisted_groups = []  # local cache for persisted groups, each group is represented by feed_id, group_id tuple
        driver_set = set()  # set of vulnerability-IDs
        item_count = 0
        changes = {}
        ex = None
        d_task_id = None
        current_state = None
        skipped = False
        driver_timer = time.time()
        try:
            namespace = _driver_modules[driver]["namespace"]
            feedtype = _driver_modules[driver]["feedtype"]
            purge_unreported = _driver_modules[driver]["purge_unreported"]
            module = _driver_modules[driver]["module"]

            try:
                # if namespace in ['centos', 'debian', 'alpine', 'ol']:
                if not str(driver_config.get(namespace, {}).get("enabled", True)).lower() in ["true", "t"]:
                    logger.info("{} driver is not enabled in config. Skipping driver execution".format(namespace))
                    skipped = True
                    continue
            except:
                logger.exception("Failed to load config for {} driver, continuing execution task with defaults".format(namespace))

            # start a task for driver execution
            d_task_id = tasks.create_driver_execution_task(feedtype, namespace, p_task_id)
            if not d_task_id:
                raise Exception("Aborting driver execution task for {}/{} due to an invalid task ID".format(feedtype, namespace))

            logger.info("Starting driver execution task {} for {}/{}".format(d_task_id, feedtype, namespace))

            with session_scope() as session:
                # fetch driver state if any and pass it to refresh op
                previous_task = (
                    session.query(Task)
                    .filter(Task.task_type == "DriverExecutionTask")
                    .filter(Task.feed_id == feedtype)
                    .filter(Task.driver_id == namespace)
                    .filter(Task.status == "completed")
                    .filter(Task.task_id < d_task_id)
                    .order_by(Task.task_id.desc())
                    .first()
                )
                previous_state = (
                    previous_task.result["execution_state"]
                    if previous_task and previous_task.result and "execution_state" in previous_task.result
                    else None
                )

                generator, current_state = module.fetch(
                    task_id=d_task_id,
                    skip_if_exists=skip_if_exists,
                    previous_state=previous_state,
                    config=driver_config.get(namespace, None),
                )

                # NOTE: Grype DB driver .fetch() returns empty list to skip this loop
                for item in generator:
                    item_timer = time.time()

                    item_count = item_count + 1

                    if item_count % 1000 == 0:
                        logger.debug("processed items ({}/{}): {}".format(feedtype, namespace, item_count))
                        #    session.flush()

                    # Collect driver output summary if purge of unreported records
                    if purge_unreported:
                        driver_set.add((item["key"], item["namespace"]))

                    # normalize payload
                    payload = common.order_payload(item["payload"], feedtype)

                    try:
                        # db lookup for this record
                        query_timer = time.time()
                        data_record = (
                            session.query(DataRecord)
                            .filter(DataRecord.feed_id == feedtype)
                            .filter(DataRecord.group_id == item["namespace"])
                            .filter(DataRecord.record_id == item["key"])
                            .one_or_none()
                        )
                        query_timer = time.time() - query_timer
                        summary_observe(
                            name=query_timer_name,
                            observation=query_timer,
                            description="Feed data record merge lookup operation",
                            driver=driver,
                            namespace=namespace,
                        )

                        if data_record:
                            # check if the payload has changed
                            if common.order_payload(data_record.payload, feedtype) != payload:
                                logger.debug(
                                    "Change detected for {}, namespace: {}. Updating db".format(item["key"], item["namespace"])
                                )
                                data_record.payload = payload
                                data_record.task_id = d_task_id
                                flush_timer = time.time()
                                session.flush()
                                flush_timer = time.time() - flush_timer
                                summary_observe(
                                    name=flush_timer_name,
                                    observation=flush_timer,
                                    description="Feed data record merge flush operation",
                                    driver=driver,
                                    namespace=namespace,
                                )

                                _increment_changes(changes, item["namespace"], updated=True)
                            else:
                                # logger.info('No change detected for {}, namespace: {}. Skipping update'.format(thing['key'], thing['namespace']))
                                pass
                        else:
                            # logger.debug('Data record for {}, namespace: {} not found. Adding to db'.format(thing['key'], thing['namespace']))
                            data_record = DataRecord(
                                feed_id=feedtype,
                                group_id=item["namespace"],
                                record_id=item["key"],
                                payload=payload,
                                task_id=d_task_id,
                            )

                            session.add(data_record)
                            flush_timer = time.time()
                            session.flush()
                            flush_timer = time.time() - flush_timer
                            summary_observe(
                                name=flush_timer_name,
                                observation=flush_timer,
                                description="Feed data record merge flush operation",
                                driver=driver,
                                namespace=namespace,
                            )
                            _increment_changes(changes, item["namespace"], created=True)

                        # lookup group record in the local cache and if not found in the db
                        # NOTE: Grype DB driver .fetch() returns empty list so the group record won't get created here.
                        if (feedtype, item["namespace"]) not in persisted_groups:
                            ns_desc = item.get("namespace_description", None)
                            if not ns_desc:
                                ns_desc = "Group record for namespace: {} and feed type: {}".format(item["namespace"], feedtype)

                            group_record = (
                                session.query(Group)
                                .filter(Group.feed_id == feedtype)
                                .filter(Group.group_id == item["namespace"])
                                .one_or_none()
                            )
                            if group_record:
                                # load it into cache
                                persisted_groups.append((feedtype, item["namespace"]))
                            else:
                                # persist the group record, should be loaded into the cache on the subsequnt run
                                logger.info(
                                    "Group record for namespace: {}, feed type: {} not found. Adding to db".format(
                                        item["namespace"], feedtype
                                    )
                                )
                                session.add(
                                    Group(
                                        feed_id=feedtype,
                                        group_id=item["namespace"],
                                        name="{} {} group".format(item["namespace"], feedtype),
                                        tier=0,
                                        description=ns_desc,
                                        task_id=d_task_id,
                                    )
                                )
                                session.flush()
                        else:
                            # parent group is in the db, nothing to do here
                            pass
                    except Exception as e:
                        logger.exception("Failed to process {}".format(item))
                        ex = e
                    finally:
                        item_timer = time.time() - item_timer
                        summary_observe(
                            name="anchore_feeds_record_processing_time",
                            observation=item_timer,
                            description="Time to process a single data record",
                            driver=driver,
                            namespace=namespace,
                        )

                        if isinstance(item, dict):
                            item.clear()

                logger.info("Total number of items output by {}/{} driver: {}".format(feedtype, namespace, item_count))

                # compare and update the items in db that are not reported by drivers that yield entire result set
                if not ex and purge_unreported and driver_set:
                    logger.debug("Comparing driver output with database state")

                    # Query all data records that match the groups
                    db_records = (
                        session.query(DataRecord.record_id, DataRecord.group_id)
                        .filter(DataRecord.feed_id == feedtype)
                        .filter(DataRecord.group_id.in_([grp for ftype, grp in persisted_groups]))
                        .all()
                    )
                    logger.debug(
                        "Database query for all records with feed_id={} and group_id in {} yielded {} results".format(
                            feedtype,
                            [grp for ftype, grp in persisted_groups],
                            len(db_records),
                        )
                    )

                    diff_map = {}
                    for record_id, group_id in set(db_records).difference(driver_set):
                        if group_id not in diff_map:
                            diff_map[group_id] = []
                        diff_map[group_id].append(record_id)

                        data_record = (
                            session.query(DataRecord)
                            .filter(DataRecord.feed_id == feedtype)
                            .filter(DataRecord.group_id == group_id)
                            .filter(DataRecord.record_id == record_id)
                            .one_or_none()
                        )
                        if data_record and data_record.payload:
                            if (
                                "Vulnerability" in data_record.payload
                                and "FixedIn" in data_record.payload["Vulnerability"]
                                and data_record.payload["Vulnerability"]["FixedIn"]
                            ):
                                logger.debug(
                                    "Change detected (not listed by driver) for {}, namespace: {}. Updating db".format(
                                        record_id, group_id
                                    )
                                )
                                payload = copy.deepcopy(data_record.payload)
                                del payload["Vulnerability"]["FixedIn"][:]
                                data_record.payload = payload
                                data_record.task_id = d_task_id
                                session.flush()
                                _increment_changes(changes, group_id, deleted=True)

                    if diff_map:
                        logger.debug(
                            "Found {} record(s) in the system that are not reported by {}/{} driver. Missing IDs by group: {}".format(
                                sum(len(cve_ids) for cve_ids in diff_map.values()),
                                feedtype,
                                namespace,
                                json.dumps(diff_map),
                            )
                        )

                    del db_records[:]
                    diff_map.clear()

                logger.info(
                    "DB stats post {}/{} driver execution: created: {}, updated: {}, (logically) deleted: {}".format(
                        feedtype,
                        namespace,
                        sum(value.get("created", 0) for value in changes.values()),
                        sum(value.get("updated", 0) for value in changes.values()),
                        sum(value.get("deleted", 0) for value in changes.values()),
                    )
                )
                logger.debug("DB change set by group: {}".format(json.dumps(changes)))
                # logger.debug('Number of new database records created: {}'.format(created))
                # logger.debug('Number of existing database records updated: {}'.format(updated))

                # Check for grand parent feed record
                if (
                    feedtype not in persisted_feeds
                    and feedtype
                    != grype_db_driver.feedtype  # TODO remove condition after grypedb driver is refactored out of this flow
                ):
                    try:
                        feed_record = session.query(Feed).filter(Feed.feed_id == feedtype).one_or_none()
                        if feed_record:
                            # load it into cache
                            persisted_feeds.append(feedtype)
                        else:
                            # persist the feed id, should be loaded into the cache on the subsequnt run
                            logger.info("Feed record for {} not found, adding it to db".format(feedtype))
                            session.add(
                                Feed(
                                    feed_id=feedtype,
                                    name="{} feed".format(feedtype),
                                    tier=0,
                                    description="Feed record for type {}".format(feedtype),
                                    task_id=d_task_id,
                                )
                            )
                            session.flush()
                    except MultipleResultsFound:
                        logger.exception("Multiple results found for feed type: {}, this is unusual".format(feedtype))
                else:
                    # grand parent feed is in the db, nothing to do here
                    pass

            logger.info("Completed driver execution task {} for {}/{}".format(d_task_id, feedtype, namespace))
        except Exception as e:
            ex = e
            logger.exception("Failed to run {}/{} driver".format(feedtype, namespace))
        finally:
            if not skipped:
                summary_observe(
                    driver_timer_name,
                    observation=time.time() - driver_timer,
                    driver=driver,
                )
                # update task statuses
                if ex:
                    # update driver execution task
                    if d_task_id:
                        tasks.update_task(
                            d_task_id,
                            status=tasks.TaskStatus.FAILED.value,
                            result={"error": str(ex)},
                        )
                        gauge_set(task_failures_name, observation=1, driver=driver)
                    # update feed sync task
                    if namespace:
                        failed_drivers.append(namespace)
                else:
                    # update driver execution task
                    if d_task_id:
                        if callable(current_state):
                            current_state = current_state()

                        tasks.update_task(
                            d_task_id,
                            status=tasks.TaskStatus.COMPLETED.value,
                            result={
                                "changes": changes,
                                "execution_state": current_state,
                            },
                        )
                        gauge_set(task_success_name, observation=1, driver=driver)
                    # update feed sync task
                    if namespace:
                        completed_drivers.append(namespace)
            else:
                skipped_drivers.append(namespace)

            # free references for garbage collection
            del persisted_groups[:]
            driver_set.clear()
            changes.clear()

    tasks.update_task(
        p_task_id,
        status=tasks.TaskStatus.COMPLETED.value,
        result={
            "completed_drivers": completed_drivers,
            "failed_drivers": failed_drivers,
            "skipped_drivers": skipped_drivers,
        },
    )

    del persisted_feeds[:]
    del completed_drivers[:]
    del failed_drivers[:]

    logger.info("Completed feed sync task {}".format(p_task_id))


def _increment_changes(change_set, group_id, created=False, updated=False, deleted=False):
    if group_id not in change_set:
        change_set[group_id] = {"created": 0, "updated": 0, "deleted": 0}

    if created:
        change_set[group_id]["created"] += 1
    if updated:
        change_set[group_id]["updated"] += 1
    if deleted:
        change_set[group_id]["deleted"] += 1


def db_lookup(namespace, payload_attribute=None):
    """
    Lookup function for fetching data records for a given namespace. JSON queries for attributes within the payload is partially supported
    #TODO make this better

    :param namespace:
    :param payload_attribute:
    :return:
    """
    try:
        with session_scope() as session:
            if payload_attribute and payload_attribute != "Vulnerability":
                q = session.query(
                    DataRecord.group_id,
                    DataRecord.record_id,
                    DataRecord.payload["Vulnerability"][payload_attribute],
                ).filter(DataRecord.group_id.like("{}%".format(namespace)))
            elif payload_attribute and payload_attribute == "Vulnerability":
                q = session.query(
                    DataRecord.group_id,
                    DataRecord.record_id,
                    DataRecord.payload["Vulnerability"],
                ).filter(DataRecord.group_id.like("{}%".format(namespace)))
            else:
                q = session.query(DataRecord.group_id, DataRecord.record_id, DataRecord).filter(
                    DataRecord.group_id.like("{}%".format(namespace))
                )

            for group_id, record_id, result in q:
                yield group_id, record_id, result
    except:
        logger.exception("Database lookup for recrods in {} namespace failed")
